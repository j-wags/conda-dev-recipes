#!/usr/bin/env python
import os
import re
import sys
import time
import shutil
import tarfile
import tempfile
import argparse
from glob import glob
from operator import add
from functools import reduce
from collections import defaultdict
from contextlib import contextmanager
from subprocess import call, check_call, CalledProcessError

try:
    # Import Conda Build API things
    import conda_build
    import conda_build.api as build_api
    from conda_build.exceptions import DependencyNeedsBuildingError
    # This might change in the future, but its functionality was stripped from the Conda API
    # at some point with no indication as why nor what replaced it
    from conda.exports import get_index
    cb_version = conda_build.__version__
    if cb_version < "3":
        raise ImportError("Conda version too low: {}".format(cb_version))

except ImportError as e:
    print('ImportError encountered:')
    print(str(e))
    print('')
    print('''This script requires conda-build >= 3 and anaconda-client. It also 
    must run in the root conda environment.

      $ conda deactivate
      $ conda install conda-build anaconda-client
      
      ''', file=sys.stderr)
    raise


BINSTAR_TOKEN = os.environ.pop('BINSTAR_TOKEN', None)

# these labels will be checked by default for existent packages.
STANDARD_LABELS = ('main',
                   'dev',
                   'rc',
                   'beta',
                   )


@contextmanager
def temporary_directory():
    """
    Create a temporary directory which will be cleaned up on close

    Python 2 compatible. Python 3.2+ can use tempfile.TemporaryDirectory to same effect
    """
    name = tempfile.mkdtemp()
    try:
        yield name
    finally:
        shutil.rmtree(name)


def is_pre_release(pkg_meta):
    """Helper function to figure out if pre-released package"""
    from pkg_resources import parse_version
    parsed = parse_version(pkg_meta.version())
    return parsed.is_prerelease


def pre_black_listed(pkg_meta):
    """Helper function to blacklist files"""
    filename = '.pre_black_listed'
    if os.path.exists(filename):
        with open(filename) as f:
            for line in f:
                if not line.startswith('#') and pkg_meta.name() in line:
                    return True
    return False


def list_package_contents(filename, protocol='bz2'):
    """
    Helper function for looking at contents of tarball
    """
    if not os.path.exists(filename):
        print('ERROR: File does not exist: {}'.format(filename))
        return

    tarf = tarfile.open(filename, 'r:{}'.format(protocol))
    print('Package contents:')
    for i, name in enumerate(tarf.getnames()):
        print('  {}'.format(name))
        if i > 20:
            print('...')
            break


def clean_builds():
    """Clean up build artifacts"""
    # Conda clean --source-cache is deprecated, this is its replacement
    # Replacing "purge" with  "purge-all" will also remove tarballs
    cmd = ['conda', 'build', 'purge', '--quiet']
    print("Cleaning up source builds...")
    call(cmd)


def upload_to_anaconda(meta, username, max_retries=5,
                       force=False, dev=False):
    """
    Upload a compiled meta package to Anaconda

    Parameters
    ----------
    meta : conda_build.metadata.MetaData
        Meta data to construct path from
    username
    max_retries
    force
    dev

    Returns
    -------

    Raises
    ------
    FileNotFoundError
        Raised when file does not exist
    """
    # Get file path
    filename = build_api.get_output_file_paths(meta)[0]
    if not os.path.exists(filename):
        raise FileNotFoundError('File does not exist: {}'.format(filename))

    cmd = ['anaconda']
    if BINSTAR_TOKEN is not None:
        cmd.extend(['-t', BINSTAR_TOKEN])
    cmd.extend(['upload', '--no-progress', '-u', username, filename])
    if force:
        cmd.extend(['--force'])
    elif meta.get_section('extra') and ('force_upload' in meta.get_section('extra')) and (
            meta.get_section('extra')['force_upload'] == 'True'):
        cmd.extend(['--force'])
    if not pre_black_listed(meta) and is_pre_release(meta):
        cmd.extend(['--label', 'dev'])
    elif dev:
        assert force, "Dev packages require forced uploads."
        cmd.extend(['--label', 'dev'])
    elif meta.get_section('extra') and 'upload' in meta.get_section('extra'):
        labels = meta.get_section('extra')['upload']
        for label in labels.split(','):
            cmd.extend(['--label', label])
    else:
        cmd.extend(['--label', 'main'])

    err = None
    for i in range(max_retries):
        try:
            print("Uploading package '{}' to anaconda user '{}'...".format(filename, username))
            check_call(cmd)
            return
        except CalledProcessError as call_err:
            err = call_err
            err.cmd = '[not shown]'
            print("Retrying after {} seconds...".format((i+1)))
            time.sleep(i+1)

    # final raise error to client
    raise err


def render_metadatas(paths, variants=None, channel_urls=(), cycle_packages=False):
    """
    Build out the incomplete Metadata's for all possible recipes we will be constructing

    Parameters
    ----------
    paths : iterable of str
        Flat iterable of paths as strings to search for conda-build directories
    variants : dict
        Variants to consider for
    channel_urls : iterable of str
        Channel URLs or Conda-style short names to search against to see if
        they are on the Anaconda.org cloud already
        Formatted as formal URLs or as shorthand such as 'omnia/label/main'
    cycle_packages : bool, default False
        Output sorted by cycling through packages instead off all variants of one package listed first

    Returns
    -------
    metas : List of conda_build.api.metadata

    """
    metas = []
    meta_lookup = {}
    # Do some pre-setup magic...
    # The variant system has 2 fatal flaws:
    # 1. If 1 variant fails for a package, they all fail
    # 2. Source files have to be downloaded to finish rendering the package
    # Solve 2 first here
    # We (Omnia) don't care about the source material, we just want the output name.
    # So long as the `build: string` does not need the git hash or something, we'd like to bypass to save space and time
    # However, setting no_download_source fails if the "work_dir" is empty as CB assumes it needs this info in case
    # someone makes a build string/jinja template a function of source material.
    # Omnia won't allow that so we can do this:
    # The work_dir = croot (Conda Build's build root) + build ID (package + epoch) + "work"
    # So we first start by tricking the initial config check that there is a populated work dir, we'll clean later
    # Caveat: This is conda_build.config:Config trickery, so this may break in the future since its not "API"
    for possible_recipe in paths:
        # Recreate the temp directory for each package because the act of cleaning up from the package removes
        # the placeholder work directory
        meta_lookup[possible_recipe] = []
        with temporary_directory() as temp_dir:  # Make a faux `croot` temporary directory
            build_id = "placeholder"  # Make a faux build_ID
            work_dir = "work"  # Make the "work_dir" directory
            work_path = os.path.join(temp_dir, build_id, work_dir)
            os.makedirs(work_path)
            with open(os.path.join(work_path, 'a_touched_file.magic'), 'w') as _:
                # Touch a random file so the "work_dir" is not empty
                pass

            try:
                if os.path.isdir(possible_recipe):
                    # Skip if no meta.yaml file
                    if not os.path.exists(os.path.join(possible_recipe, 'meta.yaml')):
                        print("Skipping {} because no meta.yaml file found".format(possible_recipe))
                        continue
                    try:
                        # This is the fastest metabuild and does 0 checks against jinja and finalization
                        meta_bundle = build_api.render(
                            possible_recipe,  # Path
                            finalize=False,  # Speeds up render
                            variants=variants,  # Parses all variants
                            bypass_env_check=True,  # Skip the env check (dont resolve anything)
                            filename_hashing=False,  # Uses old build string (--old-build-string)
                            channel_urls=channel_urls,  # Config pass-through

                            # Config-pass through no source downloading trickery (see above)
                            # all of this must be undone before leaving this function
                            # this is okay since we dont *build* any of these recipes... yet...
                            # I think this is the minimum source
                            no_download_source=True,  # Don't download anything (assume we have it)
                            keep_old_work=True,  # Don't move the work folder around, its just a placeholder for now
                            build_id=build_id,  # Force build ID to start set to this
                            croot=temp_dir  # Force croot to start set to this
                        )
                        # Flatten metadata for export
                        for variant_index, meta_tuple in enumerate(meta_bundle):
                            meta = meta_tuple[0]
                            meta.final = False  # Ensure meta is not final
                            # Revert all Config trickery changes we made to speed render
                            meta.config.keep_old_work = False  # Let work get moved around
                            meta.config.croot = None  # This resets croot to default
                            meta.config.no_download_source = False
                            # Solve Conda Build fatal problem 1: Treat each "variant" as its own "build"
                            # Because we have now undone all the Config trickery, this will ensure we get
                            # Unique, proper build ID's for each variant , treating them as independent builds
                            time.sleep(0.05)  # Add the small delay to help uniqueness (names are time.time()*1000)
                            meta.config.compute_build_id(meta.name(), reset=True)
                            # This process will make several empty build directories in your (true) conda build
                            # croot, there may be a way around that, but they are all empty and go unused.
                            # Running `conda build purge-all` will remove all build artifacts from everywhere (not just
                            # those made from this script), or just run clean_build() function
                            if not cycle_packages:
                                metas.append(meta)
                            else:
                                meta_lookup[possible_recipe].append(meta)
                    except (RuntimeError, IOError, DependencyNeedsBuildingError):
                        print('Failed to load recipe: {}'.format(possible_recipe))
                        raise
            except SystemExit:
                print("Critical ERROR: Recipe {} has a fatal error that prevents processing".format(
                    possible_recipe))
                raise
    if cycle_packages:
        while meta_lookup:  # While dict not empty
            for possible_recipe in paths:  # Cycle through recipes
                if possible_recipe in meta_lookup:  # If the recipe is in the dict
                    metas.append(meta_lookup[possible_recipe].pop(0))
                    if not meta_lookup[possible_recipe]:  # Remove recipe if empty
                        meta_lookup.pop(possible_recipe)

    return metas


def required_builds(metas, channel_urls, verbose=0, force=False, rebuild=False):
    """
    Generator to determine which builds need to be uploaded based on existing builds on Anaconda

    Checks against the following conditions to return package:
        * Package has not already been queued by this generator
        * Will force return if Package has force_upload in extra section of meta.yaml
        * Package tarball has not already been built locally (will not overwrite), unless rebuild=True
        * Package has not indicated it should be skipped
        * Package is not already on Anaconda by same name (overridden by force keyword)

    Parameters
    ----------
    metas : List of conda_build.metadata.MetaData
        Flat list of rendered conda_build metas
    channel_urls : list of string
        Channel URLs or Conda-style short names to search against to see if
        they are on the Anaconda.org cloud already
        Formatted as formal URLs or as shorthand such as 'omnia/label/main'
    verbose : int, Default: 0
        Verbosity level, higher is more verbose
        3 or greater is maximum currently
    force : bool, Default: False
        If package is on Anaconda already, force package to be built
        Distinct from ``rebuild``
    rebuild : bool, Default False
        If package is already found built locally, force it to be built
        Distinct from ``force``

    Returns
    -------
    metas : Generator of conda_build.metadata.MetaData
        Yields Metadata objects which should be built without duplication
    """

    # Get the package list from the channel URLs
    index = get_index(channel_urls=channel_urls,
                      prepend=False)
    # Sift the packages by both package name, and channel
    index_by_pkgname = defaultdict(dict)
    for k, v in index.items():
        channel = k.channel
        pkgname = k.to_filename()
        index_by_pkgname[pkgname][channel] = v

    queued_packages = {}  # Tracking already queued packages

    for meta in metas:
        # Determine package output path
        output_package = build_api.get_output_file_paths(meta)[0]
        package_base = os.path.basename(output_package)
        if verbose > 2:
            print('Evaluating whether to build package {}'.format(package_base))

        # Make sure we have not put the package in the to-build section yet
        if output_package in queued_packages:
            if verbose > 1:
                print("Skipping {} as its already been built".format(package_base))
            # Sanity check
            ref_package = queued_packages[output_package]
            if ref_package != meta and verbose > 2:  # Not sure equality differs from `is` on conda_build Metapackages
                print("Warning: Two packages were queued to build to {} but do not have the same Metas. "
                      "\nThis can happen, for example, if you have multiple NumPys, "
                      "but NumPy is not pinned so may be harmless."
                      "\nPlease check that you do not have two meta.yaml files with the same package specs in "
                      "different folders:\n"
                      "\t-{}\n"
                      "\t-{}".format(package_base, ref_package.path, meta.path))
            continue

        # Allow package recipe to force rebuild and upload in recipe.
        if meta.get_section('extra') and 'force_upload' in meta.get_section('extra') and (
                meta.get_section('extra')['force_upload'] == 'True'):
            if verbose > 1:
                print('Scheduling package due to force_upload={}: {}'.format(
                    str(meta.get_section('extra')['force_upload']), meta.name()))
                print('  full_name: {}'.format(package_base))
            yield meta
            queued_packages[output_package] = meta
            continue  # We forced package, loop now

        # Ensure its not already queued
        if not rebuild and os.path.isfile(output_package):
            if verbose > 1:
                print('Skipping because package exists locally: {}'.format(output_package))
            continue

        # The metapackage itself has indicated it should be skipped, remove from build stack
        if meta.skip():
            if verbose > 1:
                print('Skipping {}'.format(package_base))
            continue

        # Make sure package is not already on Anaconda (if we are not forcing)
        if package_base in index_by_pkgname and not force:
            if verbose > 2:
                print('Package exists on anaconda.org: {}'.format(meta.name()))
                print('  full_name:  {}'.format(package_base))
                print('  channel(s): {}'.format(tuple(index_by_pkgname[package_base].keys())))
                print('  md5:        {}'.format(tuple([v['md5'] for v in index_by_pkgname[package_base].values()])))
            continue

        if verbose > 1:
            print('Scheduling package: {}'.format(meta.name()))
            print('  full_name: {}'.format(package_base))
            print('  build variant Python: {}'.format(meta.config.variants[0].get('python', 'undefined')))
            print('  build variant NumPy: {}'.format(meta.config.variants[0].get('numpy', 'undefined')))

        yield meta
        queued_packages[output_package] = meta


def build_package(meta, args):
    """
    Build a new package and optionally upload to Anaconda from metadata

    If metadata is flagged as final, then the raw meta will be used, otherwise
    it will be reconstructed from the path.

    Parameters
    ----------
    meta : conda_build.metadata.MetaData
        Metadata to construct render from
    args : ArgumentParser
        Parsed arguments from input to set flags

    Returns
    -------
    path : string
        Absolute path to the output tarball

    """
    # Check if there is an omnia label to build against
    if meta.get_section('extra') and 'include_omnia_label' in meta.get_section('extra'):
        omnia_label = meta.get_section('extra')['include_omnia_label']
        if omnia_label not in STANDARD_LABELS:
            print('Cannot build package {} against label "{}", must be one of the standard labels: {}.\n'
                  'Falling back to using "main" only.'.format(meta.path, omnia_label, STANDARD_LABELS))
        else:
            # Add the new channel to the front (repetition does not mater, order does)
            meta.config.channel_urls = ('omnia/label/{}'.format(omnia_label),) + tuple(meta.config.channel_urls)
    # Finalize channel URLs
    meta.config.channel_urls = ('conda-forge',) + tuple(meta.config.channel_urls) + ('defaults',)
    # Build package
    kwarg_bundle = {'notest': args.notest}  # Common flags
    if meta.final:
        arg_bundle = [meta]
        # No changes to keyword arguments
    else:
        # Non-finalized, rebuild from path
        arg_bundle = [meta.path]
        kwarg_bundle['variants'] = meta.config.variants[0]
        kwarg_bundle['config'] = meta.config
    build_paths = build_api.build(*arg_bundle, **kwarg_bundle)

    for build in build_paths:
        list_package_contents(build)

    if args.upload is not None:
        if len(build_paths):  # Length 1 or larger is good
            upload_to_anaconda(meta, username=args.upload,
                               force=args.force, dev=args.dev)
        else:
            print('Package {} failed to build; will not upload.'.format(meta.dist()))

    # Clean up conda-build artifacts
    clean_builds()

    # Remove tarballs if specified
    if args.clean_tarballs:
        os.remove(build_api.get_output_file_paths(meta)[0])

    sys.stdout.flush()


def execute(args):
    """Execute the building"""
    # Parse the pythons, numpys, and cudas into long and short form
    pythons_short = re.split(',\s*|\s+', args.python)
    numpys_short = re.split(',\s*|\s+', args.numpy)
    pythons_long = [re.sub(r'^(\d)(\d)', r'\1.\2', python_spec) for python_spec in pythons_short]
    numpys_long = [re.sub(r'^(\d)(\d)', r'\1.\2', numpy_spec) for numpy_spec in numpys_short]
    if args.cuda is None:
        cudas = []
    else:
        # Cudas have a slightly different split order sequence, we operate on the short versions
        cudas_split = re.split(',\s*|\s+', args.cuda)
        cudas = [re.sub(r'^(\d+)\.?(\d\b)', r'\1\2', cuda_spec) for cuda_spec in cudas_split]

    # Compile variants, do them in reverse order because this variants system will build them in reverse
    base_variants = {'python': pythons_long[::-1], 'numpy': numpys_long[::-1], 'CUDA_SHORT_VERSION': cudas[::-1]}

    channel_urls = tuple(args.check_against) if args.check_against else ()

    if args.verbose > 0:
        print('EXECUTE:')
        print('Verbosity: {}'.format(str(args.verbose)))
        print('Pythons: {}'.format(str(pythons_short)))
        print('NumPys: {}'.format(str(numpys_short)))
        if cudas:
            print('Cudas: {}'.format(str(cudas)))
        print('channel_urls: {}'.format(str(channel_urls)))  # DEBUG

    # Flatten all paths in target
    recipe_paths = reduce(add, (glob(recipe) for recipe in args.recipe))

    if args.verbose > 0:
        print('Possible recipes provided (in random order):')
        print(recipe_paths)

    metas = render_metadatas(recipe_paths,
                             variants=base_variants,
                             channel_urls=channel_urls,
                             cycle_packages=args.cycle)

    if args.verbose > 0:
        all_paths = []
        for meta in metas:
            base_name = os.path.basename(meta.path)
            if base_name not in all_paths:
                all_paths.append(base_name)
        print('Considering recipes in the following order:')
        print(', '.join(all_paths))
        print()
    sys.stdout.flush()

    queued_metas = list(required_builds(metas, channel_urls,
                                        verbose=args.verbose,
                                        force=args.force,
                                        rebuild=args.rebuild))

    # Helpful data
    print('\n[conda-build-all] Scheduling the following {} builds:'.format(len(queued_metas)))
    for meta in queued_metas:
        print('  {}'.format(os.path.basename(build_api.get_output_file_paths(meta)[0])))
    sys.stdout.flush()

    if args.dry_run:
        print("--dry-run flag set! No builds will be executed. "
              "See return code for number of builds queued.")
        sys.exit(len(queued_metas))

    print()

    # Start builds and uploads
    failed_builds = []
    for meta in queued_metas:
        try:
            build_package(meta, args)
        except (Exception, SystemExit) as build_err:
            failed_builds.append((meta.dist(), build_err))

    if len(failed_builds) > 0:
        print(('[conda-build-all] Error: one or more packages '
               'failed to build'), file=sys.stderr)
        for build, error in failed_builds:
            print('  {}: {}'.format(build, error), file=sys.stderr)
        sys.stderr.flush()

    return len(failed_builds)


def main():
    p = argparse.ArgumentParser(
        'Build conda packages',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    p.add_argument(
        'recipe',
        action="store",
        metavar='RECIPE_PATH',
        nargs='+',
        help="path to recipe directory"
    )
    p.add_argument(
        '--check-against',
        nargs="+",
        metavar='ANACONDA_USER/label/$name',
        default=['omnia/label/{}'.format(label) for label in STANDARD_LABELS],
    )
    p.add_argument(
        '--dry-run',
        action='store_true',
        help=('Do not perform any builds. The exit status indicates the '
              'number of build to-be-performed'),
    )
    p.add_argument(
        '--upload',
        help='Automatically upload to Conda under this username',
    )
    p.add_argument(
        '--force',
        action='store_true',
        dest='force',
        help='Force a package upload regardless of errors',
    )
    p.add_argument(
        '--rebuild',
        action='store_true',
        dest='rebuild',
        help='Force a package to rebuild, even if it exists locally. Has no effect if --dry-run is set',
    )
    p.add_argument(
        '--dev',
        action='store_true',
        dest='dev',
        help=('Push package to the dev label. This requires '
              '--force to be enabled.'),
    )
    p.add_argument(
        '--no-test',
        action='store_true',
        dest='notest',
        help="do not test the package"
    )
    p.add_argument(
        '--cycle-packages',
        action="store_true",
        dest='cycle',
        help="Order builds by rotating through packages. By default all variants for one package will be attempted "
             "before any other package is attempted"
    )
    p.add_argument(
        '--python',
        help="Set the Python version used by conda build",
        metavar="PYTHON_VER",
        default='27,36,37',
    )
    p.add_argument(
        '--numpy',
        help="Set the Minimum NumPy version used by conda build",
        metavar="NUMPY_VER",
        default='1.11',
    )
    p.add_argument(
        '--cuda',
        help="Build against specific CUDA variants",
        metavar="CUDAS"
    )
    p.add_argument(
        '-c', '--clean',
        help='Remove the compiled tarballs once done. Helps keep build sizes down',
        action='store_true',
        dest='clean_tarballs'
    )
    p.add_argument(
        '-v', '--verbose',
        help='Give more output. Option is additive, and can be used up to 3 times.',
        dest='verbose',
        action='count',
    )
    args = p.parse_args()
    if args.verbose is None:
        args.verbose = 0

    if args.verbose > 2:
        print('command-line arguments:')
        print(args)

    return execute(args)


if __name__ == '__main__':
    sys.exit(main())
